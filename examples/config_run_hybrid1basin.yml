# --- Model configurations --- #
# Conceptual model to use [exphydro, ...
# Hybrid model: exphydro + mlp -> M100
# Ref: exphydro -> https://hess.copernicus.org/articles/26/5085/2022/
hybrid_model: exphydroM100
concept_model: exphydro
# Method to solve the ODEs with torchdiffeq [bosh3, rk4, dopri5, euler, adaptive_heun, midpoint]
odesmethod: euler

# Files to specify training, validation and test basins
# basin_file: 1_basin_file.txt
basin_file: 1_basin_fileB.txt

# --- Data configurations --- #
# Dataset to use for training and testing [camelsus, summaca]
data_dir: M0_results_569basins_1980_2010_RK23

# # nn_model_dir: 4basin_pretrainer_lstm_240628_080705
# # nn_model_dir: cluster1_pretrainer_lstm_500ep
# nn_model_dir: 1basin_pretrainer_lstm_01013500_240716_154241
# # nn_model_dir: ZZ1basin_pretrainer_mlp_minclip_240716_142915

# nn_model_dir: 1basin_pretrainer_lstm_270d_128x1_relu_01013500_240802_092257
# nn_model_dir: 1basin_pretrainer_lstm_270d_128x1_notRelu_01013500_240806_105256
# nn_model_dir: 1basin_pretrainer_lstm_270d_128x2_relu_01013500_240807_081817
# nn_model_dir: 1basin_pretrainer_lstm_270d_128x3_relu_01013500_240807_083924
# # nn_model_dir: 1basin_pretrainer_lstm_270d_64x1_relu_01013500_240807_083026
# nn_model_dir: 1basin_pretrainer_lstm_270d_64x2_relu_01013500_240807_083323
# nn_model_dir: 1basin_pretrainer_lstm_270d_64x6_relu_01013500_240807_154744
# nn_model_dir: 1basin_pretrainer_lstm_270d_256x1_relu_01013500_240811_165341

# nn_model_dir: 1basin_pretrainer_lstm_270d_64x2_relu_06431500_240807_194801
nn_model_dir: 1basin_pretrainer_lstm_270d_128x1_relu_06431500_240807_194925

# nn_model_dir: 1basin_pretrainer_mlp_32x5_01013500_240812_202403
# nn_model_dir: 1basin_pretrainer_mlp_32x5_01013500_240813_191809   #good
# nn_model_dir: 1basin_pretrainer_mlp_32x5_drop02_01013500_240814_105507
# nn_model_dir: 1basin_pretrainer_mlp_32x5_noClip_01013500_240814_110823  #good

# nn_model_dir: 1basin_pretrainer_mlp_32x5_06431500_240813_180815   #good
# nn_model_dir: 1basin_pretrainer_mlp_32x5_noClip_06431500_240814_111533

# # If not nn_model_dir, specify the model and parameters to use:
# ###############################################################
# # NN model to use [mlp, lstm]
# nn_model: mlp

# # # Length of the input sequence
# # seq_length: 270

# nn_dynamic_inputs:
#   - s_snow
#   - s_water
#   - prcp
#   - tmean

# nn_mech_targets:
#   - ps_bucket
#   - pr_bucket
#   - m_bucket
#   - et_bucket
#   - q_bucket

# target_variables: 
#   - obs_runoff

# # Hidden layers for the NN model
# # hidden_size:
# #   - 128

# hidden_size: 
#   # - 32
#   # - 32
#   # - 32
#   # - 32
#   # - 32

#   - 64
#   - 64
#   - 64

# # # # # # If shuffle, shuffles the batches in the data loader
# # # # # shuffle: True
# ###############################################################

scale_target_vars: True

# --- Training configurations --- #

# Period to train, validate and test the model (they should be consecutive)
train_start_date: "01/10/1980"
train_end_date: "30/09/2000"
valid_start_date: "01/10/2000"
valid_end_date: "30/09/2010"

# Loss function to use [mse, nse, nse-nh]
loss: nse

# Number of epochs to train the model
epochs: 200

# Patience for early stopping
patience: 30

# If a value, clips the gradients during training to that norm.
clip_gradient_norm: 1.0

# Batch size for training (if -1, it will be only one batch for the whole dataset)
# batch_size: 7304    # for mlp
# batch_size: 7036   # for lstm
batch_size: 256

# # Dropout rate for the NN model
# dropout: 0.2

# # Length of the input sequence
# seq_length: 270

# Optimizer to use [adam, sgd]
optimizer: adam

# # Learning rate for the optimizer
# learning_rate: 0.001
learning_rate:
  initial: 0.0001
  decay: 0.5
  decay_step_fraction: 2  # Decay step fraction (e.g., 1/4 of the total epochs

## --- Run configurations --- ##
# Experiment name, used as folder name
# experiment_name: 1basin_hybrid_lstm_01013500
# experiment_name: 1basin_hybrid_lstm_270d_128x1
# experiment_name: ZZZZZ1basin_hybrid_mlp_01013500
# experiment_name: 1basin_hybrid_lstm_270d_128x1_256b_relu_nse
# experiment_name: 1basin_hybrid_lstm_270d_256x1_256b_relu_nse
# experiment_name: 1basin_hybrid_lstm_270d_128x1_7036b_relu_nse
# experiment_name: 1basin_hybrid_lstm_270d_64x2_256b_relu_nse_bosh3

# experiment_name: ZZ1basin_hybrid_mlp_32x5_256b_nse_euler
# experiment_name: ZZ1basin_hybrid_mlp_32x5_256b_nse_euler_noClip
# experiment_name: ZZ1basin_hybrid_mlp_32x5_256b_nse_bosh3
# experiment_name: 1basin_hybrid_mlp_32x5_7304b_nse_euler
# experiment_name: ZZ1basin_hybrid_mlp_32x5_7304b_nse_euler_drop02
# experiment_name: ZZ1basin_hybrid_mlp_32x5_256b_nse_euler_noClip
# experiment_name: ZZ1basin_hybrid_mlp_32x5_256b_nse_bosh3_noClip
# experiment_name: ZZ1basin_hybrid_mlp_32x5_7304b_nse_euler_noClip
# experiment_name: ZZ1basin_hybrid_mlp_32x5_7304b_nse_bosh3_noClip

experiment_name: ZZZ1basin_hybrid_lstm_270d_128x1_256b_nse_euler_200ep
# experiment_name: ZZZ1basin_hybrid_lstm_270d_128x1_7036b_nse_euler_200ep
# experiment_name: ZZZ1basin_hybrid_lstm_270d_128x1_256b_nse_bosh3_200ep
# experiment_name: ZZZ1basin_hybrid_lstm_270d_128x1_7036b_nse_bosh3_200ep

# experiment_name: ZZZ1basin_hybrid_mlp_32x5_7304b_nse_euler_400ep
# experiment_name: ZZZ1basin_hybrid_mlp_32x5_7304b_nse_bosh3_400ep

# which GPU (id) to use [in format of cuda:0, cuda:1 etc, or cpu or None]
device: cuda:1

# Set seed for reproducibility
seed: 1111

# Set precision for the model [float32, float64]
precision: float32

# Number of parallel workers used in the data pipeline
num_workers: 16

# Verbose level [0, 1] (0: only log info messages, don't show progress bars, 1: show progress bars)
verbose: 1


# If a value and greater than 0, logs n random basins as figures after pretraining the NN model
# If list of basins, logs the specified basins: format is either '01013500' or 1013500
log_n_basins: 1
  # - 1013500
  # - 6431500

# If a value and greater than 0, logs figures and metrics, and save model after each n epochs
log_every_n_epochs: 20

# Metrics to use for evaluation (after training)
metrics:
  - NSE
  - Alpha-NSE
  - Beta-NSE
  - FHV
  - FMS
  - FLV
  - KGE
  - Beta-KGE
  - Peak-Timing
  - Peak-MAPE
  - Pearson-r