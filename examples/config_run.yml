# --- Data configurations --- #
# Dataset to use for training and testing [camelsus, summaca]
dataset: camelsus
data_dir: ../../../../../gladwell/hydrology/SUMMA/summa-ml-models/CAMELS_US

# Forcing product [daymet, maurer, maurer_extended, nldas, nldas_extended, nldas_hourly]
forcings: 
  - daymet
  # - maurer
  # - maurer_extended

nn_dynamic_inputs:
  - prcp(mm/day)
  - dayl(s)
  - tmean(C)

nn_static_inputs:
  # - lat
  # - lon
  # - elev

target_variables: 
  - obs_runoff(mm/day)

# Files to specify training, validation and test basins
# basin_file: 569_basin_file.txt
basin_file: 25_basin_file.txt

# --- Model configurations --- #
# Conceptual model to use [exphydro, ...
# Ref: exphydro -> https://hess.copernicus.org/articles/26/5085/2022/
concept_model: exphydro
nn_model: mlp

# --- Training configurations --- #

# Period to train, validate and test the model (they should be consecutive)
# train_start_date: "01/10/1980"
# train_end_date: "30/09/2000"
# valid_start_date: "01/10/2000"
# valid_end_date: "30/09/2010"
# test_start_date: "01/10/2000"
# test_end_date: "30/09/2010"

train_start_date: "01/10/1995"
train_end_date: "30/09/2000"
valid_start_date: "01/10/2000"
valid_end_date: "30/09/2002"
test_start_date: "01/10/2000"
test_end_date: "30/09/2002"

# Set if the NN model should be pre-trained
pretrain_nn: False

# Loss function to use [mse, nse]
loss: nse
# Metrics to use for evaluation
metrics:
  - NSE
  # - Alpha-NSE
  # - Beta-NSE
  # - FHV
  # - FMS
  # - FLV
  # - KGE
  # - Beta-KGE
  # - Peak-Timing
  # - Peak-MAPE
  # - Pearson-r

# Number of epochs to train the model
epochs: 100

# Batch size for training (if -1, it will be only one batch for the whole dataset)
batch_size: 256

# Optimizer to use [adam, sgd]
optimizer: adam

# Learning rate for the optimizer
learning_rate:
  initial: 0.001
  decay: 0.5
  decay_step_fraction: 4  # Decay step fraction (e.g., 1/4 of the total epochs

# Hidden layers for the NN model
hidden_layers: 
  - 32
  - 32
  - 32
  - 32
  - 32

# If a value, clips the gradients during training to that norm.
clip_gradient_norm: 1.0

# # If nn_model=lstm, -> length of the input sequence
# seq_length: 365

## --- Run configurations --- ##
# Experiment name, used as folder name
experiment_name: test_run

# which GPU (id) to use [in format of cuda:0, cuda:1 etc, or cpu or None]
device: cuda:0

# Set seed for reproducibility
seed: 1111

# Set precision for the model [float32, float64]
precision: float32

# Number of parallel workers used in the data pipeline
num_workers: 8

# Log the training loss every n steps
log_interval: 10

# If true, writes logging results into tensorboard file
log_tensorboard: True

# If a value and greater than 0, logs n random basins as figures during validation
log_n_figures: 2

# Verbose level [0, 1] (0: only log info messages, don't show progress bars, 1: show progress bars)
verbose: 1

# Save model weights every n epochs (if model performance improves on validation set)
save_weights_every: 10