{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ame805/torchHydroNodes\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from collections import OrderedDict\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "# Get the current working directory\n",
    "current_dir = Path.cwd()\n",
    "\n",
    "# Make sure code directory is in path,\n",
    "# Add the parent directory of your project to the Python path\n",
    "project_dir = str(current_dir.parent.parent.parent)\n",
    "print(project_dir)\n",
    "sys.path.append(project_dir)\n",
    "\n",
    "from src.thn_run import (\n",
    "    _load_cfg_and_ds,\n",
    "    get_basin_interpolators,\n",
    "    get_calibration_dataset,\n",
    ")\n",
    "\n",
    "from src.modelzoo_concept import get_concept_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg_file = \"examples/config_run_calibrate_test.yml\"\n",
    "input_vars = ['prcp', 'tmean', 'dayl']\n",
    "output_vars = ['obs_runoff']\n",
    "\n",
    "# HIDDEN_LAYERS = [64, 64, 64, 64, 64]\n",
    "# HIDDEN_LAYERS = [32, 32, 32, 32, 32]\n",
    "HIDDEN_LAYERS = [128, 128, 128, 128, 128]\n",
    "\n",
    "# LR = 1e-3\n",
    "LR = {\n",
    "    \"initial\": 0.01,\n",
    "    \"decay\": 0.5,\n",
    "    \"decay_step_fraction\": 2,\n",
    "}\n",
    "\n",
    "EPOCHS = 400\n",
    "COLL_FRACT = 0.6 # fraction of time series to use as collocaiton points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classes and functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HydrologyPINN(nn.Module):\n",
    "    \"\"\"\n",
    "    A PyTorch neural network model for hydrology-based physics-informed neural networks (PINNs).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_size, output_size, \n",
    "                 hidden_layers=HIDDEN_LAYERS, \n",
    "                 params_bounds=None, scaler=None):\n",
    "        \"\"\"\n",
    "        Initialize the neural network with variable hidden layers.\n",
    "        \n",
    "        Args:\n",
    "            input_size (int): The number of input features.\n",
    "            output_size (int): The number of output features.\n",
    "            hidden_layers (list of int): A list where each element represents the number of neurons in each hidden layer.\n",
    "            params_bounds (list of tuples): A list of tuples where each tuple represents the lower and upper bounds for each parameter\n",
    "        \"\"\"\n",
    "        super(HydrologyPINN, self).__init__()\n",
    "\n",
    "        # Save the scaler\n",
    "        self.scaler = scaler\n",
    "        \n",
    "        # Set up the neural network layers\n",
    "        layers = [nn.Linear(input_size, hidden_layers[0]), nn.Tanh()]\n",
    "        \n",
    "        # Add hidden layers\n",
    "        for i in range(1, len(hidden_layers)):\n",
    "            layers.append(nn.Linear(hidden_layers[i-1], hidden_layers[i]))\n",
    "            layers.append(nn.Tanh())\n",
    "        \n",
    "        # Add output layer\n",
    "        layers.append(nn.Linear(hidden_layers[-1], output_size))\n",
    "        self.network = nn.Sequential(*layers)\n",
    "        \n",
    "        # Now initialize parameters with the same order guaranteed\n",
    "        self.params = nn.ParameterDict(OrderedDict({\n",
    "            name: nn.Parameter(\n",
    "                torch.Tensor(1).uniform_(bounds[0], bounds[1]), requires_grad=True\n",
    "            ) for name, bounds in params_bounds.items()\n",
    "        }))\n",
    "        self.params_bounds = params_bounds  # Store bounds for clamping\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Scale the input if a scaler is provided\n",
    "        if self.scaler is not None:\n",
    "            x = torch.tensor(self.scaler.transform(x.cpu().numpy()), dtype=x.dtype, device=x.device)\n",
    "        return self.network(x)\n",
    "\n",
    "    def get_clamped_params(self):\n",
    "        \"\"\"\n",
    "        Return parameters clamped within their bounds.\n",
    "        \"\"\"\n",
    "        return {name: torch.clamp(param, *self.params_bounds[name]) for name, param in self.params.items()}\n",
    " \n",
    "def setup_optimizer_and_scheduler(model, lr=LR, epochs=EPOCHS):\n",
    "    \"\"\"\n",
    "    Set up the optimizer and scheduler for the model.\n",
    "    \n",
    "    Args:\n",
    "        model (nn.Module): The PyTorch model to optimize.\n",
    "        lr (float or dict): The initial learning rate for the optimizer or a dictionary with scheduler parameters.\n",
    "        epochs (int): Total number of training epochs, used to set the scheduler's step size.\n",
    "    \n",
    "    Returns:\n",
    "        optimizer (torch.optim.Optimizer): The optimizer for the model.\n",
    "        scheduler (torch.optim.lr_scheduler._LRScheduler or None): The learning rate scheduler, if applicable.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Check if lr is a scalar (float or int)\n",
    "    if isinstance(lr, (float, int)):\n",
    "        # Optimizer with scalar learning rate\n",
    "        optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "        scheduler = None  # No scheduler for scalar lr\n",
    "\n",
    "    else:\n",
    "        # Optimizer with initial learning rate from lr dictionary\n",
    "        optimizer = optim.Adam(model.parameters(), lr=lr[\"initial\"])\n",
    "\n",
    "        # Scheduler with decay based on epochs and decay_step_fraction\n",
    "        step_size = max(1, epochs // lr[\"decay_step_fraction\"])  # Ensure step_size is at least 1\n",
    "        scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=step_size, gamma=lr[\"decay\"])\n",
    "\n",
    "    return optimizer, scheduler\n",
    "\n",
    "# Define the data loss and physics-based loss functions\n",
    "def data_loss(predicted, observed):\n",
    "\n",
    "    return nn.MSELoss()(predicted, observed)\n",
    "\n",
    "def physics_loss(model, basin, predicted_params, observed, collocation_indices):\n",
    "    \"\"\"\n",
    "    Calculate the physics-based loss on collocation points.\n",
    "    \n",
    "    Args:\n",
    "        model: The ODE model for simulation.\n",
    "        basin: The current basin being processed.\n",
    "        predicted_params: Parameters predicted by the PINN.\n",
    "        observed (torch.Tensor): Observed data tensor.\n",
    "        collocation_indices (torch.Tensor): Indices for collocation points.\n",
    "    \n",
    "    Returns:\n",
    "        torch.Tensor: The calculated physics loss.\n",
    "    \"\"\"\n",
    "    # Run the model simulation and get the full time series\n",
    "    simulated = model.run(basin, basin_params=predicted_params, use_grad=True)\n",
    "    runoff_sim = simulated[-1]  # Extract the runoff component from the model output\n",
    "    \n",
    "    # Subset observed and simulated data using collocation indices\n",
    "    observed_colloc = torch.index_select(observed, 0, collocation_indices)\n",
    "    runoff_sim_colloc = torch.index_select(runoff_sim, 0, collocation_indices)\n",
    "\n",
    "    # Calculate the physics-based penalty for the collocation points\n",
    "    physics_penalty = runoff_sim_colloc - observed_colloc\n",
    "\n",
    "    return torch.mean(physics_penalty ** 2), runoff_sim\n",
    "\n",
    "def pinn_loss(predicted, observed, predicted_params, model, basin, epoch, collocation_indices):\n",
    "    dataloss = data_loss(predicted, observed)\n",
    "    physicsloss, simulated_ode = physics_loss(model, basin, predicted_params, observed, collocation_indices)\n",
    "    icloss = nn.MSELoss()(predicted[0], observed[0])\n",
    "\n",
    "    print(f'DataLoss: {dataloss:.3e}, PhysicsLoss: {physicsloss:.3e}, IC Loss: {icloss:.3e}')\n",
    "    plot_results(observed, predicted, simulated_ode, basin, epoch, period='train') \n",
    "\n",
    "    return dataloss + physicsloss + icloss\n",
    "\n",
    "def plot_results(observed, predicted, simulated_ode, basin, epoch, period='train'):\n",
    "\n",
    "    nse_nn  = nse_loss(predicted, observed)\n",
    "    nse_ode = nse_loss(simulated_ode, observed)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "    plt.plot(observed, label='Observed', color='blue')\n",
    "    plt.plot(predicted.detach().cpu().numpy(), label=f'Predicted_NN  (NSE: {nse_nn:.3f})',\n",
    "             color='red', linestyle='--')\n",
    "    # plt.plot(simulated_ode.detach().cpu().numpy(), label=f'Simulated_ODE (NSE: {nse_ode:.3f})',\n",
    "    #          color='green', linestyle=':')\n",
    "    plt.scatter(range(len(simulated_ode)), simulated_ode.detach().cpu().numpy(), \n",
    "                    label=f'Simulated_ODE (NSE: {nse_ode:.3f})', color='green', \n",
    "                    marker='o', s=10)\n",
    "    \n",
    "    # Plot a marker on the first value of simulated ODE\n",
    "    plt.scatter(0, simulated_ode.detach().cpu().numpy()[0], color='k', marker='X', s=50)\n",
    "    \n",
    "    plt.legend()\n",
    "    plt.xlabel('Time Step')\n",
    "    plt.ylabel('Values')\n",
    "    plt.title(f'Observed vs. Predicted | {period} | epoch {epoch}')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{basin}_pins_simulation.png')\n",
    "    plt.clf()\n",
    "\n",
    "def nse_loss(predicted, observed):\n",
    "\n",
    "    return 1 - torch.sum((predicted - observed) ** 2) / (torch.sum((observed - torch.mean(observed)) ** 2) + torch.finfo(torch.float32).eps)\n",
    "\n",
    "def get_collocation_indices(total_length, coll_fraction=0.6, seed=42):\n",
    "    \"\"\"\n",
    "    Generate random indices for collocation points based on a fraction of the total time series,\n",
    "    ensuring that the points are distributed over the entire domain.\n",
    "    \n",
    "    Args:\n",
    "        total_length (int): Total length of the time series.\n",
    "        coll_fraction (float): Fraction of time series to use as collocation points.\n",
    "        seed (int): Random seed for reproducibility.\n",
    "        \n",
    "    Returns:\n",
    "        torch.Tensor: Tensor containing the random indices for collocation points.\n",
    "    \"\"\"\n",
    "    # Set the random seed for reproducibility\n",
    "    random.seed(seed)\n",
    "    \n",
    "    # Calculate the number of collocation points based on the fraction\n",
    "    num_collocation_points = int(total_length * coll_fraction)\n",
    "    \n",
    "    # Randomly sample indices across the full range of the time series\n",
    "    collocation_indices = random.sample(range(total_length), num_collocation_points)\n",
    "    \n",
    "    # Sort indices to maintain order in time (optional)\n",
    "    collocation_indices.sort()\n",
    "    \n",
    "    print(f'Number of collocation points: {num_collocation_points}')\n",
    "    print(f'Max index: {max(collocation_indices)}')\n",
    "    print(f'Min index: {min(collocation_indices)}')\n",
    "\n",
    "    return torch.tensor(collocation_indices, dtype=torch.long)\n",
    "\n",
    "##################################################\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Loading the config file and the dataset\n",
      "-- Using device: cuda:0 --\n",
      "Setting seed for reproducibility: 1111\n",
      "-- Loading basin dynamics into xarray data set.\n",
      "100%|██████████| 4/4 [00:00<00:00, 11.92it/s]\n"
     ]
    }
   ],
   "source": [
    "cfg, dataset = _load_cfg_and_ds(Path(project_dir) / cfg_file, model='conceptual')\n",
    "\n",
    "# Get the basin interpolators\n",
    "interpolators = get_basin_interpolators(dataset, cfg, project_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loop over basins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total length of time series: 1827\n",
      "Number of collocation points: 1096\n",
      "Max index: 1826\n",
      "Min index: 0\n",
      "DataLoss: 6.906e+00, PhysicsLoss: 2.436e+01, IC Loss: 1.852e-04\n",
      "Epoch 1/400, Loss: 31.263368606567383, params: [421.1609, 1217.3712, 0.2192, 1329.0988, 168.825, 1.8139, 4.6648, -5.4678]\n",
      "DataLoss: 4.111e+00, PhysicsLoss: 2.876e+01, IC Loss: 1.904e+00\n",
      "Epoch 2/400, Loss: 34.77211380004883, params: [421.1609, 1217.3712, 0.2292, 1329.0887, 168.835, 1.8239, 4.6548, -5.4778]\n",
      "DataLoss: 1.092e+01, PhysicsLoss: 2.387e+01, IC Loss: 3.076e-01\n",
      "Epoch 3/400, Loss: 35.102359771728516, params: [421.1609, 1217.3712, 0.2219, 1329.0808, 168.842, 1.8217, 4.6546, -5.486]\n",
      "DataLoss: 4.538e+00, PhysicsLoss: 2.350e+01, IC Loss: 3.862e+00\n"
     ]
    }
   ],
   "source": [
    "for basin in tqdm(dataset.basins, disable=cfg.disable_pbar, file=sys.stdout):\n",
    "    # # Skip basins that have already been processed\n",
    "    # if basin in processed_basins:\n",
    "    #     print(f\"Basin {basin} already processed, skipping.\")\n",
    "    #     continue\n",
    "\n",
    "    ds_calib, time_idx0 = get_calibration_dataset(cfg, dataset, basin)\n",
    "    \n",
    "    time_idx0 = 0\n",
    "    model_concept = get_concept_model(cfg, ds_calib, interpolators, time_idx0,\n",
    "                                        dataset.scaler, odesmethod=cfg.odesmethod)\n",
    "    \n",
    "    # Get the input and output sizes\n",
    "    input_size = len(input_vars)\n",
    "    output_size = len(output_vars)\n",
    "    \n",
    "    # Get the parameter bounds\n",
    "    params_bounds = OrderedDict(model_concept.cfg.params_bounds)\n",
    "\n",
    "    # DS to DF\n",
    "    df_calib = ds_calib.to_dataframe()\n",
    "\n",
    "    # Get the input and output data\n",
    "    input_data = torch.tensor(df_calib[input_vars].values, dtype=torch.float32)\n",
    "    observed_data = torch.tensor(df_calib[output_vars].values, dtype=torch.float32)\n",
    "    # Scale the input data\n",
    "    scaler = StandardScaler().fit(input_data)\n",
    "    # input_data = scaler.fit_transform(input_data)\n",
    "    # # To tensor\n",
    "    # input_data = torch.tensor(input_data, dtype=cfg.precision['torch'])\n",
    "\n",
    "    # Crethe PINN model\n",
    "    model_pinn = HydrologyPINN(input_size, output_size, \n",
    "                               hidden_layers=HIDDEN_LAYERS, \n",
    "                               params_bounds=params_bounds,\n",
    "                               scaler=scaler)\n",
    "    \n",
    "    # optimizer = optim.Adam(model_pinn.parameters(), lr=LR)\n",
    "    optimizer, scheduler = setup_optimizer_and_scheduler(model_pinn, lr=LR, epochs=EPOCHS)\n",
    "\n",
    "   # Generate collocation indices based on the length of the time series for this basin\n",
    "    total_length = len(ds_calib.obs_runoff)\n",
    "    print(f\"Total length of time series: {total_length}\")\n",
    "    collocation_indices = get_collocation_indices(total_length, COLL_FRACT, \n",
    "                                                  seed=model_concept.cfg.seed)\n",
    "\n",
    "    # Train the model\n",
    "    for epoch in range(EPOCHS):\n",
    "\n",
    "        model_pinn.train()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass: Simulate data through the network\n",
    "        predicted_data = model_pinn(input_data)\n",
    "\n",
    "        # # Update the plot with new predicted values\n",
    "        # plot_results(observed_data, predicted_data, basin, epoch+1)\n",
    "\n",
    "        # Clamp the parameters within their bounds\n",
    "        predicted_params = model_pinn.get_clamped_params()\n",
    "\n",
    "        # Extract the parameters from the model\n",
    "        basin_params = [predicted_params[param] for param in predicted_params.keys()]\n",
    "\n",
    "        # Calculate the loss\n",
    "        loss = pinn_loss(predicted_data, observed_data, basin_params, \n",
    "                         model_concept, basin, epoch+1, collocation_indices)\n",
    "\n",
    "        # Backward pass: Compute the gradient of the loss with respect to model parameters\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Update the learning rate if a scheduler is provided\n",
    "        if scheduler is not None:\n",
    "            scheduler.step()\n",
    "            \n",
    "        # Convert basin_params to list if it's a PyTorch tensor, then extract and round values\n",
    "        params_list = (\n",
    "            [round(param.item(), 4) for param in basin_params] if isinstance(basin_params, torch.Tensor) else\n",
    "            [round(param.item(), 4) if isinstance(param, torch.Tensor) else round(param, 4) for param in basin_params]\n",
    "        )\n",
    "\n",
    "        # Print the result\n",
    "        print(f\"Epoch {epoch+1}/{EPOCHS}, Loss: {loss.item()}, params: {params_list}\")\n",
    "\n",
    "    # Save the model\n",
    "    torch.save(model_pinn.state_dict(), f\"{basin}_pinn_model.pth\")\n",
    "    print(f\"Model saved for basin {basin}\")\n",
    "\n",
    "    # Save the scaler\n",
    "    torch.save(scaler, f\"{basin}_scaler.pth\")\n",
    "    print(f\"Scaler saved for basin {basin}\")\n",
    "\n",
    "    # Save the parameters as csv\n",
    "    params_df = pd.DataFrame(params_list, columns=['value'])\n",
    "    params_df.to_csv(f\"{basin}_params.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv-hydronodes",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
